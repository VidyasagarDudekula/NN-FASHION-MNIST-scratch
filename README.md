# Fashion MNIST Classification ğŸ‘—ğŸ‘Ÿ

> **From Scratch Neural Network to CNN: A Deep Learning Evolution**

This project demonstrates the power of convolutional neural networks by comparing a **custom-built fully connected neural network** against a **modern CNN architecture** on the Fashion MNIST dataset. Both implementations achieve strong results, with the CNN delivering a significant accuracy boost.

---

## ğŸ¯ Results at a Glance

| Model | Test Accuracy | Test Loss | Architecture |
|:------|:-------------:|:---------:|:-------------|
| **Fully Connected NN** (from scratch) | 87.48% | 0.2177 | 784 â†’ 300 â†’ 100 â†’ 10 |
| **CNN** (PyTorch) | **93.19%** | 0.2689 | Conv2D + Deep FC + Dropout + LayerNorm |

### ğŸ“ˆ Model Evolution & Performance Journey

| Version | Model | Accuracy | Loss | What Changed | Why It Helped |
|:-------:|:------|:--------:|:----:|:-------------|:--------------|
| **v0** | Fully Connected NN | 87.48% | 0.2177 | *Baseline* â€” from scratch with NumPy | â€” |
| **v1** | CNN (Basic) | 91.73% | 0.2783 | Added Conv2D layers + BatchNorm + MaxPool | Spatial feature extraction captures patterns FC layers miss |
| **v2** | CNN + Deep FC Head | 92.54% | 0.2860 | Added 2Ã— hidden FC layers (1024) + LayerNorm | Deeper classification head learns more complex decision boundaries |
| **v3** | CNN + Full Regularization | **93.19%** | **0.2689** | Added Dropout(0.1) to FC layers | Prevents overfitting, improves generalization on test data |

```diff
  v0 (FC NN)     â†’ 87.48%  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–Œ
+ v1 (CNN)       â†’ 91.73%  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–Œ (+4.25%)
+ v2 (Deep FC)   â†’ 92.54%  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–Œ (+0.81%)
+ v3 (Dropout)   â†’ 93.19%  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â–Œ (+0.65%)
```

**Total improvement: +5.71% accuracy** through iterative architectural enhancements.

---

## ğŸ“‰ Training Loss Curve (CNN)

![Training vs Validation Loss](training_loss_plot.png)

The loss curve shows:
- **Rapid convergence** in the first ~500 steps
- **Excellent generalization** â€” validation loss stays below training loss throughout
- **No overfitting** â€” thanks to aggressive dropout regularization across all layers
- **Final validation loss** settles around ~0.20

---

## ğŸ§  Model Architectures

### 1. Fully Connected Neural Network (`train.py`)
*Built completely from scratch using only NumPy and SciPy*

```
Input (784) â†’ Hidden1 (300, ReLU) â†’ Hidden2 (100, ReLU) â†’ Output (10, Sigmoid)
```

**Key Features:**
- âœ… Manual forward/backward propagation
- âœ… Hand-coded ReLU derivatives
- âœ… Stochastic Gradient Descent (SGD)
- âœ… No deep learning frameworks

**Highlights:**
- Implements **backpropagation from scratch** using the chain rule
- Uses **ReLU activations** in hidden layers to prevent vanishing gradients
- Achieves **87.48% accuracy** with pure NumPy math

---

### 2. Convolutional Neural Network (`cnn_train.py`)
*Built with PyTorch for GPU-accelerated training*

```
Input (1Ã—28Ã—28)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONVOLUTIONAL FEATURE EXTRACTOR                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Conv2D (64 filters, 3Ã—3, padding=1)                        â”‚
â”‚      â†’ ReLU â†’ BatchNorm2D â†’ MaxPool (2Ã—2) â†’ Dropout(0.1)    â”‚
â”‚                                                             â”‚
â”‚  Conv2D (128 filters, 3Ã—3)                                  â”‚
â”‚      â†’ ReLU â†’ BatchNorm2D â†’ MaxPool (2Ã—2) â†’ Dropout(0.1)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
  Flatten (128Ã—6Ã—6 = 4608)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FULLY CONNECTED CLASSIFICATION HEAD                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Linear (4608 â†’ 1024) â†’ ReLU â†’ LayerNorm â†’ Dropout(0.1)     â”‚
â”‚  Linear (1024 â†’ 1024) â†’ ReLU â†’ LayerNorm â†’ Dropout(0.1)     â”‚
â”‚  Linear (1024 â†’ 10)   [Output logits]                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- âœ… 2 Convolutional layers with increasing filter depth (64 â†’ 128)
- âœ… Batch Normalization after conv layers for training stability
- âœ… **2 Hidden fully connected layers** (1024 neurons each)
- âœ… **Layer Normalization** after FC layers for improved gradient flow
- âœ… **Dropout (0.1) on ALL layers** â€” conv blocks AND FC layers
- âœ… AdamW optimizer with learning rate 3e-4
- âœ… Cross-Entropy loss function
- âœ… MPS/CUDA acceleration support

**Highlights:**
- Achieves **93.19% accuracy** on the test set
- Trains for 10 epochs with 90/10 train-validation split
- Comprehensive dropout regularization prevents overfitting
- Layer normalization enables stable training of deeper networks

---

## ğŸ“‚ Project Structure

```
fashion-mnist-scratch/
â”œâ”€â”€ train.py                 # From-scratch fully connected NN
â”œâ”€â”€ cnn_train.py             # PyTorch CNN implementation
â”œâ”€â”€ data_preprocessor.py     # Data loading & normalization utilities
â”œâ”€â”€ normal_nn_stats.json     # FC network evaluation results
â”œâ”€â”€ cnn_model_stats.json     # CNN evaluation results
â”œâ”€â”€ training_loss_plot.png   # Loss curve visualization
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Train the Fully Connected Network
```bash
python train.py
```

### Train the CNN
```bash
python cnn_train.py
```

*Requires Fashion MNIST CSV files (`fashion-mnist_train.csv`, `fashion-mnist_test.csv`) in the project directory.*

---

## ğŸ”¬ Why the CNN Wins

| Aspect | Fully Connected NN | CNN |
|--------|-------------------|-----|
| **Spatial Awareness** | Treats pixels independently | Learns local patterns (edges, textures) |
| **Parameter Efficiency** | ~266K params (dense) | Shared conv filters + deep FC head |
| **Translation Invariance** | âŒ No | âœ… Yes |
| **Feature Hierarchy** | Flat representation | Lowâ†’High level features |
| **Normalization** | None | BatchNorm + LayerNorm |
| **Regularization** | None | **Dropout on ALL layers** |

The CNN architecture combines:
1. **Convolutional feature extraction** â€” captures spatial patterns efficiently
2. **Deep fully connected head** â€” provides powerful non-linear classification
3. **Aggressive regularization** â€” dropout on both conv and FC layers prevents overfitting
4. **Modern normalization** â€” BatchNorm + LayerNorm for stable, fast training

---

## ğŸ“Š Dataset

**Fashion MNIST** consists of 70,000 grayscale images (28Ã—28 pixels) across 10 clothing categories:

| Label | Class |
|:-----:|:------|
| 0 | T-shirt/top |
| 1 | Trouser |
| 2 | Pullover |
| 3 | Dress |
| 4 | Coat |
| 5 | Sandal |
| 6 | Shirt |
| 7 | Sneaker |
| 8 | Bag |
| 9 | Ankle boot |

---

## ğŸ› ï¸ Dependencies

- **NumPy** & **SciPy** (for from-scratch NN)
- **PyTorch** (for CNN)
- **Matplotlib** (for loss visualization)

---

<p align="center">
  <i>Built to understand neural networks from the ground up ğŸ§ª</i>
</p>
